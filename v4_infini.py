#!/usr/bin/env python3
# RÃ¢S-Fr v4.0 â€“ âˆž params virtuels, 4 bits, < 6 GB VRAM
import torch, torch.nn.functional as F, time, hashlib
from torch import nn

DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
print(f"ðŸŽ¯ Infini sur {DEVICE}")

# ---------- config ----------
RING_SIZE   = 2**32                   # 4 milliards de buckets
GROUP_4B    = 64
CACHE_MAX_MO = 5_000                  # 5 GB VRAM
NÅ’UDS_ACTIFS = 4                      # 4 actifs par token

# ---------- quantif 4 bits (idem v3) ----------
def quantif_group_4bits(x: torch.Tensor) -> tuple:
    groupe = GROUP_4B
    mini, maxi = x.view(-1, groupe).min(1).values, x.view(-1, groupe).max(1).values
    scale = (maxi - mini) / 15
    zero  = mini
    x_int = ((x.view(-1, groupe) - zero.unsqueeze(1)) / scale.unsqueeze(1)).round().clamp(0, 15)
    x_int = x_int.to(torch.uint8)
    x_int = x_int[:, ::2] << 4 | x_int[:, 1::2]
    return x_int, scale, zero

def dequantif_4bits(data: tuple) -> torch.Tensor:
    x_int, scale, zero = data
    x1 = (x_int >> 4) & 0xF
    x2 = x_int & 0xF
    x_int = torch.stack([x1, x2], dim=-1).view(-1)
    return x_int * scale.repeat_interleave(GROUP_4B) + zero.repeat_interleave(GROUP_4B)

# ---------- micro-dÃ©codeur ----------
class MicroDecodeur(nn.Module):
    def __init__(self):
        super().__init__()
        self.mlp = nn.Sequential(nn.Linear(1, 64), nn.GELU(), nn.Linear(64, 1))
    def forward(self, x):
        return self.mlp(x.unsqueeze(-1)).squeeze(-1)

micro = MicroDecodeur().half().to(DEVICE)

# ---------- nÅ“ud virtuel infini ----------
class NoeudInfini:
    def __init__(self, ident: int):
        self.ident = ident

    def bucket(self) -> int:
        """Bucket circulaire 0 -> RING_SIZE-1"""
        return int(hashlib.blake2b(str(self.ident).encode(), digest_size=4).hexdigest(), 16) % RING_SIZE

    def materialiser(self, device: str):
        # graine = bucket pour reproductibilitÃ©
        w1 = torch.randn(1024, 4096, device=device, dtype=torch.float16)
        w2 = torch.randn(4096, 1024, device=device, dtype=torch.float16)
        w1 = micro(w1.view(-1, 1)).reshape(1024, 4096)
        w2 = micro(w2.view(-1, 1)).reshape(4096, 1024)
        return quantif_group_4bits(w1), quantif_group_4bits(w2)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        w1_data, w2_data = self.materialiser(x.device)
        w1 = dequantif_4bits(w1_data)
        w2 = dequantif_4bits(w2_data)
        return F.silu(x @ w1) @ w2

# ---------- meta-routeur infini ----------
class MetaInfini(nn.Module):
    def __init__(self):
        super().__init__()
        self.emb = nn.Linear(256, 2048)
        self.blocs = nn.TransformerEncoder(
            nn.TransformerEncoderLayer(2048, 32, 8192, batch_first=True), num_layers=12
        )
        self.head = nn.Linear(2048, 1000)  # 1000 logits fictifs

    def forward(self, x):
        h = self.emb(x)
        h = self.blocs(h)
        logits = self.head(h[:, 0, :])
        # on gÃ©nÃ¨re **identifiants infinis** Ã  partir des logits
        seed = int((logits.sum().item() * 1e6) % RING_SIZE)
        return [seed + i for i in range(500)]  # 500 identifiants uniques

meta = MetaInfini().half().to(DEVICE)

# ---------- cache infini ----------
class CacheInfini:
    def __init__(self, max_mo: int = 5000):
        self.max_bytes = max_mo * 1024 * 1024
        self.cache = {}  # bucket -> (w1, w2) 4 bits
        self.cles = []

    def _bytes(self, bucket: int) -> int:
        return 2 * 1024 * 4096  # â‰ˆ 8 MB

    def get(self, ident: int, device: str):
        bucket = NoeudInfini(ident).bucket()
        if bucket in self.cache:
            self.cles.remove(bucket); self.cles.append(bucket)
            return self.cache[bucket]
        while self.cles and (len(self.cles) * self._bytes(0) > self.max_bytes):
            vieux = self.cles.pop(0); del self.cache[vieux]
        noeud = NoeudInfini(ident)
        w1, w2 = noeud.materialiser(device)
        self.cache[bucket] = (w1, w2)
        self.cles.append(bucket)
        return w1, w2

cache_infini = CacheInfini()

# ---------- embedding + head ----------
VOCAB_SIZE = 32003
embedding = nn.Embedding(VOCAB_SIZE, 256).half().to(DEVICE)
tete_lm   = nn.Linear(1024, VOCAB_SIZE, bias=False).half().to(DEVICE)

# ---------- gÃ©nÃ©ration âˆž ----------
@torch.no_grad()
def generer_infini(prompt: str, max_tokens: int = 30, temperature: float = 1.0):
    ids = torch.tensor([hash(c) % VOCAB_SIZE for c in prompt[:64]], device=DEVICE).unsqueeze(0)
    for _ in range(max_tokens):
        emb = embedding(ids).mean(dim=1).unsqueeze(1)  # (1,1,256)
        idents = meta(emb)  # 500 identifiants uniques
        hidden = emb.repeat(1, 4, 1).view(4, 256)
        for rank, ident in enumerate(idents[:4]):
            w1, w2 = cache_infini.get(ident, DEVICE)
            w1_d = dequantif_4bits(w1)
            w2_d = dequantif_4bits(w2)
            hidden[rank] = F.silu(hidden[rank] @ w1_d) @ w2_d
        out = hidden.mean(dim=0, keepdim=True)
        logits_lm = tete_lm(out)
        nxt = torch.multinomial(torch.softmax(logits_lm / temperature, dim=-1), num_samples=1)
        ids = torch.cat([ids, nxt], dim=1)
    return ids[0].tolist()

# ---------- bench ----------
def bench_infini():
    prompt = "La quantification 4 bits permet"
    torch.cuda.synchronize()
    t0 = time.time()
    out = generer_infini(prompt, max_tokens=50)
    torch.cuda.synchronize()
    tok_s = 50 / (time.time() - t0)
    print(">>>", out[:20])  # 20 tokens
    print(f"âš¡ âˆž params virtuels â€“ tok/s : {tok_s:.1f}")
    print(f"ðŸ”Œ VRAM max : {torch.cuda.max_memory_allocated()/1e9:.2f} GB")

# ---------- main ----------
if __name__ == "__main__":
    bench_infini()